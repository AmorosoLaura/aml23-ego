{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2125209de034f326",
   "metadata": {},
   "source": [
    "### Colab instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdeb2ad5636f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/GioC1810/aml23-ego.git\n",
    "!pip install omegaconf coloredlogs wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd103ec49d743a3",
   "metadata": {},
   "source": [
    "## Epic kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbd56a11a4d043",
   "metadata": {},
   "source": [
    "#### Feature extraction script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbc7a388f05bb2",
   "metadata": {},
   "source": [
    "The script extract the features for 5, 10 and 25 num frames per clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27164192932022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "splits=(\"test\")\n",
    "num_frames=(5)\n",
    "dense_sampling=(\"True\")\n",
    "\n",
    "for split_type in ${splits[@]}\n",
    "do\n",
    "  for frames in ${num_frames[@]}\n",
    "  do\n",
    "    for sampling in ${dense_sampling[@]}\n",
    "    do\n",
    "        python3 save_feat.py \\\n",
    "            config=configs/I3D_save_feat.yaml \\\n",
    "            dataset.shift=D1-D1 \\\n",
    "            save.num_frames_per_clip.RGB=$frames \\\n",
    "            save.dense_sampling.RGB=$sampling \\\n",
    "            split=$split_type \\\n",
    "            dataset.RGB.data_path=./ek_data/frames \n",
    "    done\n",
    "  done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f507c86877d6c74",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32241bbc653b2fe5",
   "metadata": {},
   "source": [
    "Indicate:\n",
    "- shift \n",
    "- num frames per clip  \n",
    "- model \n",
    "- model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cda3bf11ff251",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd aml23-ego\n",
    "\n",
    "python3 train_classifier_ek.py name=model_name \\\n",
    "  config=configs/default.yaml \\\n",
    "  dataset.shift=D1-D1 \\\n",
    "  train.num_frames_per_clip.RGB=25 \\\n",
    "  train.dense_sampling.RGB=False \\\n",
    "  models.RGB.model=MLP_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becac1540863443",
   "metadata": {},
   "source": [
    "## Action net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f6841c4192ef0",
   "metadata": {},
   "source": [
    "#### Action net frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ffmpeg -i action-net/video/S04_video.mp4 -vf \"fps=30,scale=456:256\" -q:v 2 action-net/frames/S04_1/frame_%010d.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ac95cbad15530",
   "metadata": {},
   "source": [
    "#### Feature rgb extraction\n",
    "Indicate:\n",
    "- num frames per clip\n",
    "- split (train or test)\n",
    "- dense sampling (True or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7a82f5923f8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "splits=(\"train\" \"test\")\n",
    "num_frames=(5 10 25)\n",
    "dense_sampling=(\"True\")\n",
    "\n",
    "for split_type in ${splits[@]}\n",
    "do\n",
    "  for frames in ${num_frames[@]}\n",
    "  do\n",
    "    for sampling in ${dense_sampling[@]}\n",
    "    do\n",
    "        python3 save_feat_action_net.py \\\n",
    "            config=configs/feature_rgb_extraction.yaml \\\n",
    "            save.num_frames_per_clip.RGB=$frames \\\n",
    "            save.dense_sampling.RGB=$sampling \\\n",
    "            split=$split_type\n",
    "    done\n",
    "  done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d81c9cbf27a62",
   "metadata": {},
   "source": [
    "#### Decompress emg features train\n",
    "\n",
    "In order to push the features we compressed them, this script decompress them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887140ea2ee6a9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T14:20:31.107328Z",
     "start_time": "2024-04-24T14:20:28.275240Z"
    }
   },
   "outputs": [],
   "source": [
    "import lzma\n",
    "\n",
    "with lzma.open('saved_features_an_multimodal/features_emg_allData_train.pkl.xz', 'rb') as f_in:\n",
    "    with open('saved_features_an_multimodal/features_emg_allData_train.pkl', 'wb') as f_out:\n",
    "        f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce084d7eb3589a",
   "metadata": {},
   "source": [
    "#### Split emg spectogram features in order to push it\n",
    "\n",
    "This is the script to split the features in 3 different files in order to push to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3d0661e70becf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:03:35.261118Z",
     "start_time": "2024-04-26T20:03:29.403763Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def split_dict(filename, output_prefix):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    length = len(data['features'])\n",
    "    third_length = length // 3\n",
    "\n",
    "    split = [data['features'][0:third_length], data['features'][third_length:2*third_length], data['features'][2*third_length:length]]\n",
    "\n",
    "    for i in range(1,4):\n",
    "        with open(output_prefix + f'_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(split[i-1], f)\n",
    "\n",
    "split_dict('saved_features_an/saved_features_an_emg_10s/features_emg_10s_spectrogram_allData_train.pkl', 'saved_features_an/saved_features_an_emg_10s/features_emg_10s_spectrogram_train_allData_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d10489a0c05d89",
   "metadata": {},
   "source": [
    "#### Decompress and join emg spectogram features\n",
    "\n",
    "This is the script to execute in order to obtain the joined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1379477bc51d351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:14:19.797582Z",
     "start_time": "2024-04-26T20:14:13.449256Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def join_spectogram_feature(filename, output_name):\n",
    "    joined_features = {'features': []}\n",
    "    \n",
    "    for i in range(3):\n",
    "        with open(filename + f'_{i+1}.pkl', 'rb') as f:\n",
    "            joined_features['features'].extend(pickle.load(f))\n",
    "            \n",
    "    \n",
    "    with open(output_name, 'wb') as f:\n",
    "        pickle.dump(joined_features, f)\n",
    "        \n",
    "join_spectogram_feature('saved_features_an_multimodal/features_emg_spectrogram_train_split', \n",
    "                        'saved_features_an_multimodal/features_emg_spectrogram_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab59b32",
   "metadata": {},
   "source": [
    "#### Benchmarking EMG\n",
    "\n",
    "To test the different performance of the EMG classifier according to the different combinations of the EMG preprocessing parameters, and to avoid computing locally the features and push them on github here the code to compute them on colab. \n",
    "\n",
    "- Preprocessing\n",
    "- Reformatting the features as the loader expects them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e816d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd aml23-ego/EMG\n",
    "python EMG_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b713316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_emg_features(full_data, split, spectrogram):\n",
    "    #os.chdir('aml23-ego/EMG/')\n",
    "    full_data = pd.read_pickle(full_data)\n",
    "    print(len(full_data))\n",
    "    \n",
    "    #full_data = full_data[full_data['file'] == 'S04_1.pkl']\n",
    "    full_data = full_data.rename(columns={'emg_data': 'features_EMG'})\n",
    "\n",
    "    emg_features = full_data[\n",
    "        ['uid', 'features_EMG']]\n",
    "    emg_features = emg_features.to_dict(orient='index')\n",
    "    emg_features = {'features': list(emg_features.values())}\n",
    "    print(emg_features['features'][0]['features_EMG'].shape)\n",
    "    features_name = f'../saved_features_an_multimodal/features_emg_spectogram_allData_{split}.pkl' if spectrogram \\\n",
    "        else f'../saved_features_an_multimodal/features_emg_10fs_10s_30seg_allData_{split}.pkl'\n",
    "\n",
    "    with open(features_name, 'wb') as f:\n",
    "        pickle.dump(emg_features, f)\n",
    "\n",
    "create_emg_features(full_data=\"./new_emg_data_10fs_10s_30seg_train.pkl\", split='train', spectrogram=False)\n",
    "create_emg_features(full_data=\"./new_emg_data_10fs_10s_30seg_test.pkl\", split='test', spectrogram=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4ea77e6012e6d",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1be341fa7282d3",
   "metadata": {},
   "source": [
    "###### RGB model\n",
    "Indicate:\n",
    "- Dense sampling: to choose which features the model use for training\n",
    "- num frames per clip: to choose which features the model use for training\n",
    "- model: which model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ed977d2987778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "num_frames=(5 10 25)\n",
    "dropout=(0 0.6)\n",
    "sub_sample_num=(2 3 4)\n",
    "\n",
    "for frames in ${num_frames[@]}\n",
    "do\n",
    "  for sub_sample in ${sub_sample_num[@]}\n",
    "  do\n",
    "    for drop in ${dropout[@]}\n",
    "    do\n",
    "        python3 train_classifier.py name=TRN_drop${drop}_feature_${frames} \\\n",
    "            config=configs/training_rgb.yaml \\\n",
    "            train.dense_sampling.RGB=True \\\n",
    "            train.num_frames_per_clip.RGB=$frames \\\n",
    "            models.RGB.model=TRN_classifier \\\n",
    "            models.RGB.dropout=$drop \\\n",
    "            models.RGB.subsample_num=$sub_sample \\\n",
    "            models.RGB.num_layers=0\n",
    "    done\n",
    "  done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb49a647312aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "num_frames=(5 10 25)\n",
    "dropout=(0 0.6)\n",
    "\n",
    "for frames in ${num_frames[@]}\n",
    "do\n",
    "  for drop in ${dropout[@]}\n",
    "  do\n",
    "        python3 train_classifier.py name=LSTM_drop${drop}_feature_${frames} \\\n",
    "            config=configs/training_rgb.yaml \\\n",
    "            train.dense_sampling.RGB=True \\\n",
    "            train.num_frames_per_clip.RGB=$frames \\\n",
    "            models.RGB.model=Lstm_classifier \\\n",
    "            models.RGB.dropout=$drop \\\n",
    "            models.RGB.subsample_num=0 \\\n",
    "            models.RGB.num_layers=1\n",
    "  done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd aml23-ego\n",
    "python3 train_classifier_an_rgb.py name=train_model_rgb_lstm_5s \\\n",
    "  config=configs/training_rgb_an.yaml \\\n",
    "  train.dense_sampling.RGB=True \\\n",
    "  train.num_frames_per_clip.RGB=10 \\\n",
    "  dataset.annotations_path=an_annotations/an_annotations_multimodal_5s \\\n",
    "  dataset.RGB.data_path=saved_features_an/saved_features_an_rgb_5s \\\n",
    "  models.RGB.model=Lstm_classifier \\\n",
    "  models.RGB.dropout=0 \\\n",
    "  models.RGB.subsample_num=None \\\n",
    "  models_dir=saved_models \\\n",
    "  models.RGB.num_layers=1 \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036d4d8faf5f887",
   "metadata": {},
   "source": [
    "###### EMG model\n",
    "\n",
    "Indicate:\n",
    "- spectogram_feat: To choose which kind of features the model is train on\n",
    "- model: according to the kind of modality \n",
    "    - spectogram_feat = True -> LeNet5\n",
    "    - spectogram_feat = False -> EMG_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a11d4a5e82c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd aml23-ego\n",
    "\n",
    "python3 train_classifier_multimodal.py name=train_cnn_emg_80fs\\\n",
    "  config=configs/training_emg.yaml \\\n",
    "  train.spectogram_feat=True \\\n",
    "  models.EMG.lr=0.01 \\\n",
    "  models.EMG.weight_decay=1e-6 \\\n",
    "  dataset.EMG.features_name=features_emg_80fs\\\n",
    "  dataset.annotations_path=an_annotations \\\n",
    "  dataset.EMG.data_path=saved_features_an \\\n",
    "  models.EMG.model=LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5fda69a888d6b",
   "metadata": {},
   "source": [
    "#### Multimodal models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb5014ce9682cd",
   "metadata": {},
   "source": [
    "##### Late fusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f7a6ddf5c03aa",
   "metadata": {},
   "source": [
    "###### Testing\n",
    "\n",
    "Indicate:\n",
    "- Dense sampling: to choose which features the model use for training\n",
    "- num frames per clip: to choose which features the model use for training\n",
    "- model: which model to use (both RGB and EMG)\n",
    "- spectogram_feat: To choose which kind of features the model is train on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a343b4d4f7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd aml23-ego\n",
    "python3 train_classifier_multimodal.py name=multimodal_late \\\n",
    "  config=configs/late_fusion_test.yaml \\\n",
    "  train.dense_sampling.RGB=True \\\n",
    "  train.num_frames_per_clip.RGB=10 \\\n",
    "  train.spectogram_feat=False \\\n",
    "  models.RGB.model=Lstm_classifier \\\n",
    "  models.EMG.model=EMG_LSTM \\\n",
    "  dataset.annotations_path=an_annotations/an_annotations_multimodal_5s \\\n",
    "  dataset.RGB.data_path=saved_features_an/saved_features_an_rgb_5s \\\n",
    "  dataset.EMG.data_path=saved_features_an/saved_features_an_emg_5s \\\n",
    "  resume_from.RGB=saved_models/ActionSense/RGB_LSTM_5s_10frame/action-classifier_RGB_9.pth \\\n",
    "  resume_from.EMG=saved_models/ActionSense/EMG_LSTM_5s_new/action-classifier_EMG_9.pth \\\n",
    "  dataset.EMG.features_name=features_emg_5s \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014e16224fe3ea5",
   "metadata": {},
   "source": [
    "##### Midl level fusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3368a41064636",
   "metadata": {},
   "source": [
    "###### Mid level feature extraction\n",
    "\n",
    "Indicate: \n",
    "- Dense sampling: to choose which features the model use for training\n",
    "- num frames per clip: to choose which features the model use for training\n",
    "- split (train or test)\n",
    "- model: which model to use (both RGB and EMG)\n",
    "- spectogram_feat: To choose which kind of features the model is train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a78f161f7f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd aml23-ego\n",
    "python3 save_feat_midlevel_multimodal.py name=midlevel_feat \\\n",
    "  config=configs/mid_level_extraction.yaml \\\n",
    "  split=test \\\n",
    "  save.dense_sampling.RGB=True \\\n",
    "  save.num_frames_per_clip.RGB=10 \\\n",
    "  save.spectrogram_feat=False \\\n",
    "  models.RGB.model=Lstm_classifier \\\n",
    "  models.EMG.model=EMG_LSTM \\\n",
    "  dataset.annotations_path=an_annotations/an_annotations_multimodal_5s \\\n",
    "  dataset.RGB.data_path=saved_features_an/saved_features_an_rgb_5s \\\n",
    "  dataset.EMG.data_path=saved_features_an/saved_features_an_emg_5s \\\n",
    "  resume_from.RGB=saved_models/ActionSense/RGB_LSTM_5s_10frame/action-classifier_RGB_9.pth \\\n",
    "  resume_from.EMG=saved_models/ActionSense/EMG_LSTM_5s_new/action-classifier_EMG_9.pth \\\n",
    "  dataset.EMG.features_name=features_emg_5s \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9598edbd932b1",
   "metadata": {},
   "source": [
    "###### Training\n",
    "\n",
    "Indicate: \n",
    "- Dense sampling: to choose which features the model use for training\n",
    "- num frames per clip: to choose which features the model use for training\n",
    "- model: which model to use (both RGB and EMG)\n",
    "- spectogram_feat: To choose which kind of features the model is train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75331fccbedba134",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd aml23-ego\n",
    "python3 train_midlevel_classifier.py name=midlevel_classifier_ZZZ_lstm_lstm_5s_max \\\n",
    "  config=configs/multimodal_midLevel_training.yaml \\\n",
    "  train.dense_sampling.RGB=True \\\n",
    "  train.num_frames_per_clip.RGB=10 \\\n",
    "  train.spectogram_feat=False \\\n",
    "  dataset.annotations_path=an_annotations/an_annotations_multimodal_5s \\\n",
    "  models.hidden_size=256 \\\n",
    "  models.FUSION.lr=0.01 \\\n",
    "  models.FUSION.weight_decay=1e-7\\\n",
    "  train.num_iter=800 \\\n",
    "  train.lr_steps=500 \\\n",
    "  models.FUSION.lr_steps=500 \\\n",
    "  #dataset.FUSION.features_name=midlevel_feat_lstm_lstm_5s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
