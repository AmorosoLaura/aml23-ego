{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7288622a8212497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T09:40:17.609896Z",
     "start_time": "2024-04-27T09:40:15.175075Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "activities_to_classify = {\n",
    "        'Get/replace items from refrigerator/cabinets/drawers': 0,\n",
    "        'Peel a cucumber': 1,\n",
    "        'Clear cutting board': 2,\n",
    "        'Slice a cucumber': 3,\n",
    "        'Peel a potato': 4,\n",
    "        'Slice a potato': 5,\n",
    "        'Slice bread': 6,\n",
    "        'Spread almond butter on a bread slice': 7,\n",
    "        'Spread jelly on a bread slice': 8,\n",
    "        'Open/close a jar of almond butter': 9,\n",
    "        'Pour water from a pitcher into a glass': 10,\n",
    "        'Clean a plate with a sponge': 11,\n",
    "        'Clean a plate with a towel': 12,\n",
    "        'Clean a pan with a sponge': 13,\n",
    "        'Clean a pan with a towel': 14,\n",
    "        'Get items from cabinets: 3 each large/small plates, bowls, mugs, glasses, sets of utensils': 15,\n",
    "        'Set table: 3 each large/small plates, bowls, mugs, glasses, sets of utensils': 16,\n",
    "        'Stack on table: 3 each large/small plates, bowls': 17,\n",
    "        'Load dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils': 18,\n",
    "        'Unload dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils': 19,\n",
    "}\n",
    "\n",
    "FPS = 30\n",
    "ACTION_LENGTH = 5\n",
    "\n",
    "def augmentation(data_frame):\n",
    "    augmented_data = []\n",
    "\n",
    "    for _, row in data_frame.iterrows():\n",
    "        start_frame = row['start_frame']\n",
    "        stop_frame = row['stop_frame']\n",
    "        interval_size = FPS * ACTION_LENGTH \n",
    "\n",
    "        num_intervals = math.ceil((stop_frame - start_frame + 1) / interval_size)\n",
    "\n",
    "        for i in range(num_intervals):\n",
    "            new_start = start_frame + i * interval_size\n",
    "            new_stop = min(new_start + interval_size - 1, stop_frame)  \n",
    "            new_row = row.copy()\n",
    "            new_row['start_frame'] = new_start\n",
    "            new_row['stop_frame'] = new_stop\n",
    "            new_row['start_timestamp'] = new_start/FPS\n",
    "            new_row['stop_timestamp'] = new_stop/FPS\n",
    "            augmented_data.append(new_row)\n",
    "\n",
    "    augmented_dataframe = pd.DataFrame(augmented_data, columns=data_frame.columns)\n",
    "    augmented_dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return augmented_dataframe\n",
    "\n",
    "def create_annotations_file(timestamp_file, annotations_file, type='train'):\n",
    "\n",
    "    timestamps = pd.read_pickle(timestamp_file)\n",
    "    timestamps = timestamps.drop(\n",
    "        ['myo_left_timestamps', 'myo_right_timestamps', 'myo_left_readings', 'myo_right_readings'], axis=1)\n",
    "    timestamps = timestamps.reset_index()\n",
    "\n",
    "    start_timestamp = timestamps['start'].iloc[0]\n",
    "    timestamps['start_frame'] = ((timestamps['start'] - start_timestamp) * FPS).astype(int)\n",
    "    timestamps['stop_frame'] = ((timestamps['stop'] - start_timestamp) * FPS).astype(int)\n",
    "    \n",
    "    annotations = pd.read_pickle(annotations_file)\n",
    "    annotations = annotations[annotations['file'] == 'S04_1.pkl']\n",
    "    annotations = annotations.drop(['labels'], axis=1)\n",
    "    \n",
    "    complete_labels = pd.merge(timestamps, annotations, on='index', how='inner')\n",
    "    \n",
    "    complete_labels['uid'] = complete_labels['index']\n",
    "    complete_labels['participant_id'] = 'S04'\n",
    "    complete_labels['video_id'] = 'S04_1'\n",
    "    complete_labels['verb'] = complete_labels['description_x']\n",
    "    complete_labels['narration'] = complete_labels['description_x']\n",
    "    complete_labels['verb_class'] = complete_labels['verb'].map(activities_to_classify)\n",
    "\n",
    "    complete_labels = complete_labels[\n",
    "        ['uid', 'participant_id', 'video_id', 'narration', 'start', 'stop', 'start_frame',\n",
    "         'stop_frame', 'verb', 'verb_class']]\n",
    "    \n",
    "    complete_labels['type'] = type\n",
    "    \n",
    "    return complete_labels\n",
    "\n",
    "def change_uid_to_emg(emg_data, split):\n",
    "    emg_data = pd.read_pickle(emg_data)\n",
    "    emg_data['uid'] = emg_data.reset_index().index + 1\n",
    "    emg_data.to_pickle(f'new_emg_data_{split}.pkl')    \n",
    "\n",
    "\n",
    "def take_S04_annotations_RGB(timestamps, emg_data, type):\n",
    "    \n",
    "    calibration_val = pd.read_pickle(timestamps)['start'].iloc[0]\n",
    "\n",
    "    emg_data = pd.read_pickle(emg_data)\n",
    "    \n",
    "    emg_data = emg_data[emg_data['file'] == 'S04_1.pkl']\n",
    "    \n",
    "    emg_data = emg_data.rename(columns={'file': 'video_id', 'description': 'narration', 'description_class': 'verb_class'})\n",
    "    emg_data['participant_id'] = 'S04'\n",
    "    emg_data['video_id'] = 'S04_1'\n",
    "    emg_data['start_frame'] = ((emg_data['start'] - calibration_val) * FPS).astype(int)\n",
    "    emg_data['stop_frame'] = ((emg_data['stop'] - calibration_val) * FPS).astype(int)\n",
    "    emg_data['verb'] = emg_data['narration']\n",
    "    emg_data = emg_data.drop(['emg_data'], axis = 1)\n",
    "    emg_data = emg_data[\n",
    "        ['uid', 'participant_id', 'video_id', 'narration', 'start', 'stop', 'start_frame',\n",
    "         'stop_frame', 'verb', 'verb_class']]\n",
    "    \n",
    "    emg_data.to_pickle(f'an_annotations_rgb/S04_{type}.pkl')    \n",
    "\n",
    "def create_reduced_annotations(train_annotations, test_annotations):\n",
    "    \n",
    "    combined_df = pd.concat([train_annotations, test_annotations], ignore_index=True)\n",
    "    \n",
    "    combined_df = combined_df.sample(frac=1)\n",
    "\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    combined_df['uid'] = combined_df.index\n",
    "    \n",
    "    train_df_final = combined_df[combined_df['type'] == 'train']\n",
    "    test_df_final = combined_df[combined_df['type'] == 'test']\n",
    "    \n",
    "    train_df_final = train_df_final.drop(['type'], axis=1)\n",
    "    test_df_final = test_df_final.drop(['type'], axis=1)\n",
    "\n",
    "    train_df_final.to_pickle(f\"an_annotations_rgb/S04_train.pkl\")    \n",
    "    test_df_final.to_pickle(f\"an_annotations_rgb/S04_test.pkl\")   \n",
    "    \n",
    "def create_multimodal_annotations(full_data, split, spectogram):\n",
    "    \n",
    "    full_data = pd.read_pickle(full_data)\n",
    "    full_data = full_data[full_data['file'] == 'S04_1.pkl']\n",
    "    full_data = full_data.rename(columns={'description_class': 'verb_class'})\n",
    "    full_data['participant_id'] = 'S04'\n",
    "    full_data['video_id'] = 'S04_1'\n",
    "    \n",
    "    create_emg_features(deepcopy(full_data), split, spectogram)\n",
    "    \n",
    "    final_annotations = full_data[\n",
    "        ['uid', 'participant_id', 'video_id', 'description', 'verb_class']]\n",
    "    \n",
    "    final_annotations.to_pickle(f\"an_multimodal_annotations/S04_{split}.pkl\")   \n",
    "\n",
    "def create_emg_features(full_data, split, spectogram):\n",
    "    \n",
    "    full_data = full_data[full_data['file'] == 'S04_1.pkl']\n",
    "    full_data = full_data.rename(columns={'emg_data': 'features_EMG'})\n",
    "    \n",
    "    emg_features = full_data[\n",
    "        ['uid', 'features_EMG']]\n",
    "    emg_features = emg_features.to_dict('list')\n",
    "    emg_features = {'features': emg_features}\n",
    "\n",
    "    features_name = f'saved_features_an_multimodal/features_emg_spectogram_S04_{split}.pkl' if spectogram \\\n",
    "        else f'saved_features_an_multimodal/features_emg_S04_{split}.pkl'\n",
    "      \n",
    "    with open(features_name, 'wb') as f:\n",
    "        pickle.dump(emg_features, f)\n",
    "          \n",
    "def create_emg_spec_features(full_data, split, spectrogram):\n",
    "    \n",
    "    full_data = pd.read_pickle(full_data)\n",
    "    \n",
    "    full_data = full_data.rename(columns={'spectrogram': 'features_EMG_spectrogram'})\n",
    "    \n",
    "    print(split)\n",
    "    emg_features = full_data[\n",
    "        ['uid', 'features_EMG_spectrogram']]\n",
    "    emg_features= emg_features.to_dict(orient='index')\n",
    "    emg_features = {'features': list(emg_features.values())}\n",
    "    # Creazione del dizionario desiderato\n",
    "\n",
    "    features_name = f'saved_features_an_multimodal/features_emg_80fs_spectrogram_allData_{split}.pkl' if spectrogram \\\n",
    "        else f'saved_features_an_multimodal/features_emg_80fs_allData_{split}.pkl'\n",
    "      \n",
    "    with open(features_name, 'wb') as f:\n",
    "        pickle.dump(emg_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12888eac96f2059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T09:40:57.164940Z",
     "start_time": "2024-04-27T09:40:57.072990Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "take_S04_annotations_RGB(timestamps='action-net/S04_1.pkl', emg_data='EMG_data/emg/new_emg_data_test.pkl', type='test')\n",
    "take_S04_annotations_RGB(timestamps='action-net/S04_1.pkl', emg_data='EMG_data/emg/new_emg_data_train.pkl', type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8375367fedd5dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T18:46:23.276885Z",
     "start_time": "2024-04-16T18:46:23.217221Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_multimodal_annotations(full_data=\"new_emg_data_train.pkl\", split='train', spectogram=False)\n",
    "create_multimodal_annotations(full_data=\"new_emg_data_test.pkl\", split='test', spectogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_emg_features(full_data=\"new_emg_data_train.pkl\", split='train')\n",
    "create_emg_features(full_data=\"new_emg_data_test.pkl\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d13b1a4f66adc26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T17:08:55.028127Z",
     "start_time": "2024-04-16T17:08:54.959225Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "create_emg_spec_features(full_data=\"C:/Users/Laura/Desktop/Universita/Polito/Advanced Machine Learning/aml23-ego/EMG_data/emg_spectrogram_80fs_test.pkl\", spectrogram=True, split='test')\n",
    "create_emg_spec_features(full_data=\"C:/Users/Laura/Desktop/Universita/Polito/Advanced Machine Learning/aml23-ego/EMG_data/emg_spectrogram_80fs_train.pkl\", spectrogram=True, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('new_emg_data_top_test.pkl', 'rb') as f_pickle:\n",
    "      dati=pickle.load(f_pickle)\n",
    "      print(dati)\n",
    "      df=pd.DataFrame(dati)\n",
    "      \n",
    "      # print(df.to_string(index=False, justify='right'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"C:/Users/Laura/Desktop/Universita/Polito/Advanced Machine Learning/aml23-ego/EMG_data/emg_spectrogram_80fs_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd925c1a72951cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T14:46:47.828822Z",
     "start_time": "2024-05-11T14:46:47.821557Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data2 = pd.read_pickle('C:/Users/Laura/Desktop/Universita/Polito/Advanced Machine Learning/aml23-ego/saved_features_an_multimodal/features_emg_spectrogram_80fs_allData_test.pkl')\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8535c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(pd.read_pickle('C:/Users/Laura/Desktop/Universita/Polito/Advanced Machine Learning/aml23-ego/EMG_data/emg_spectrogram_80fs_test.pkl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "db49ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['spectrogram', 'label', 'uid', 'file'], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [spectrogram, label, uid, file]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.columns)\n",
    "duplicated_uids = df[df['uid'].duplicated(keep=False)]\n",
    "# This will give you a DataFrame containing only rows with duplicate uids\n",
    "print(duplicated_uids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
